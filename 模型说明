# 项目简介——猫狗大战
    - 源于kaggle上的一个竞赛项目
    - 利用给定的图像数据集，用算法实现猫和狗的识别


# 迁移学习
    - 利用已经训练好的模型，作为新模型训练的初始化的学习方式
    - 优势：
        - 所需的样本数量更少
        - 模型达到收敛耗时更短
            - 比如网络在cifar-10数据集上迭代训练5000次收敛，将一个在cifar-100上训练好的模型迁移至cifar-10上，只需要1000次迭代就可以收敛
    - 什么时候适合选择迁移学习
        - 当新数据集比较小，且与原数据集相似时
        - 当算力比较有限时
    - 如何使用
        - 当新数据集比较小且和原数据集相似时，只训练softmax层
            - 把原模型的softmax层换成自己的softmax层，冻结前面所有层的参数
        - 当新数据集不是非常小，而且我们具有一定的算力，可以训练部分参数
            - 除了softmax层的调整外，还可以改变模型的后面几层（冻结浅层的参数，训练较深层的参数）
        - 具备足够的样本和算力，可以对所有参数进行训练，原来模型的参数则作为初始化的数据
    - 迁移学习的规律：
        - 随着数据的增加和算力的增加，需要冻结的层数越来越少，参与训练的层数越来越多
        - 计算机视觉是经常要用到迁移学习的领域

# 项目实践：用VGG16模型进行迁移学习
    ## 数据准备
        - 数据集包括猫和狗的图片各12500张，测试集包括12500张猫狗图片
    ## VGG16的TensorFlow的实现
        - 定义功能函数
        - 定义VGG16模型类
    ## VGG16模型复用
        - 微调(finetuining)：对需要训练的少量层级和相关参数进行重新训练
            - trainable参数变动：
                - 在进行微调对模型进行重新训练时，对于部分不需要训练的层可以通过设置trainable=False，来确保其在训练过程中不会被修改权值
                - 在VGG16类中调整相关代码
            - 全连接层的神经元个数：预训练的VGG是在ImageNet数据集上进行的，对1000个类别进行判定，若希望利用已训练模型用于其他分类任务，需要修改最后的全连接层
        - 载入权重：复用已经训练好的权重参数，通过load方法将数据以字典的形式读入
    ## 数据输入
        - data_in_func：数据输入、预处理等相关函数
    ## 模型重新训练与保存
    ## 预测